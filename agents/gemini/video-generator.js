import fs from 'fs';
import path from 'path';
import config from '../../config/config.js';
import geminiClient from '../../utils/gemini-client.js';
import { batchConcurrent } from '../../utils/utils.js';

const VIDEO_STYLE = 'Cocomelon style: bright vibrant colors, simple cute character design, smooth 3D animation, child-friendly visual style, rounded friendly characters, clear lines, simple backgrounds, educational and entertaining, playful and cheerful atmosphere, smooth motion from keyframe A to keyframe B, high quality, consistent style and visual continuity';

class VideoGeneratorAgent {
  // åˆ›å»ºç´ ææ•°æ®ç»“æ„
  _createMaterial(keyframe) {
    const { shot, keyframeA, keyframeB } = keyframe;
    return {
      shotNumber: shot.shotNumber,
      timeRange: shot.timeRange,
      startTime: shot.startTime,
      endTime: shot.endTime,
      type: 'video',
      path: null,
      status: 'pending',
      shot,
      keyframeA,
      keyframeB
    };
  }

  // è·å–é•œå¤´æ—¶é—´æ®µå†…çš„æ‰€æœ‰å¡ç‚¹
  _getBeatPointsInRange(shot, musicAnalysis) {
    const beatPoints = musicAnalysis?.beatPoints || [];
    const shotStart = shot.startTime;
    const shotEnd = shot.endTime;
    
    return beatPoints.filter(beat => beat >= shotStart && beat < shotEnd);
  }

  // æ„å»ºè§†é¢‘æç¤ºè¯
  _buildPrompt(shot, keyframeA, keyframeB, storyboard) {
    const duration = shot.endTime - shot.startTime;
    const beatPointsInRange = this._getBeatPointsInRange(shot, storyboard?.musicAnalysis);
    const rhythm = storyboard?.musicAnalysis?.rhythm;
    const concept = storyboard?.visualConcept?.visualConcept;
    
    // æ„å»ºèŠ‚æ‹åŒæ­¥æè¿°
    const buildBeatSyncDescription = () => {
      if (beatPointsInRange.length > 0) {
        const relativeBeatTimes = beatPointsInRange.map(beat => ({
          absolute: beat,
          relative: beat - shot.startTime
        }));
        const beatTimesAbsolute = relativeBeatTimes.map(b => `${b.absolute.toFixed(2)}s`).join(', ');
        const beatTimesRelative = relativeBeatTimes.map(b => `${b.relative.toFixed(2)}s`).join(', ');
        return [
          `Beat points in this segment (at music time): ${beatTimesAbsolute}`,
          `Beat points relative to segment start: ${beatTimesRelative}`,
          `At these beat points (${beatTimesRelative}), the action or camera movement MUST emphasize or change to match the music rhythm`,
          `The motion should accelerate, change direction, or create visual emphasis at these exact moments to sync with the music beats`,
          `Visual rhythm must match musical rhythm - action peaks should align with beat points`
        ];
      } else if (shot.beatPoint != null) {
        const relativeBeatTime = shot.beatPoint - shot.startTime;
        return [
          `Beat point at ${relativeBeatTime.toFixed(2)}s into this segment (music time: ${shot.beatPoint.toFixed(2)}s)`,
          `At this beat point (${relativeBeatTime.toFixed(2)}s), emphasize the action or change camera movement to sync with the music beat`
        ];
      }
      return [];
    };

    // å¿…é¡»ä½¿ç”¨ videoPromptï¼Œå¦‚æœæ²¡æœ‰åˆ™æŠ›å‡ºé”™è¯¯
    if (!shot.videoPrompt) {
      throw new Error(`é•œå¤´ ${shot.shotNumber} ç¼ºå°‘å¿…éœ€çš„ videoPrompt å­—æ®µ`);
    }
    
    const parts = [
      // ä½¿ç”¨æä¾›çš„ videoPrompt ä½œä¸ºåŸºç¡€ï¼ˆæè¿°åŠ¨æ€åŠ¨ä½œï¼‰
      shot.videoPrompt,
      
      // æ·»åŠ é¦–å°¾å¸§è¿‡æ¸¡ä¿¡æ¯
      `Generate video from keyframe A to keyframe B:`,
      `Start from keyframe A (shot ${shot.shotNumber} start)`,
      keyframeB.nextShotNumber 
        ? `transition smoothly to keyframe B (shot ${keyframeB.nextShotNumber} start)`
        : `transition smoothly to keyframe B (shot ${shot.shotNumber} end)`,
      
      // æ—¶é—´å’ŒåŒæ­¥
      `duration: ${duration} seconds`,
      `time range: ${shot.timeRange} (music time: ${shot.startTime.toFixed(2)}s - ${shot.endTime.toFixed(2)}s)`,
      `CRITICAL: This video segment corresponds to music time ${shot.startTime.toFixed(2)}s - ${shot.endTime.toFixed(2)}s`,
      `The video motion, action, and rhythm MUST sync with the music beat and rhythm`,
      
      // èŠ‚æ‹åŒæ­¥æè¿°
      ...buildBeatSyncDescription(),
      
      // åŒæ­¥ç‚¹
      shot.syncPoint && `Sync point: ${shot.syncPoint}`,
      
      // éŸ³ä¹èŠ‚å¥ä¿¡æ¯
      rhythm?.bpm && `Music BPM: ${rhythm.bpm} - video motion tempo should match this beat rate`,
      rhythm?.bpm && `Beat interval: ${(60 / rhythm.bpm).toFixed(2)} seconds - motion should follow this rhythm`,
      rhythm?.character && `Music rhythm character: ${rhythm.character} - video motion should reflect this rhythm style`,
      
      // Cocomelon é£æ ¼
      VIDEO_STYLE,
      `The video motion rhythm must match the music rhythm throughout the entire segment`
    ].filter(Boolean);
    
    return parts.join(', ');
  }

  // å‡†å¤‡å…³é”®å¸§å›¾åƒè·¯å¾„
  _prepareReferenceImages(keyframeA, keyframeB) {
    const images = [];
    if (keyframeA?.path && fs.existsSync(keyframeA.path)) {
      images.push(keyframeA.path);
    }
    if (keyframeB?.path && fs.existsSync(keyframeB.path)) {
      images.push(keyframeB.path);
    }
    return images;
  }

  // ç”Ÿæˆå•ä¸ªè§†é¢‘
  async _generateVideo(material, keyframeData) {
    const { shot, keyframeA, keyframeB } = material;
    
    console.log(`  ğŸ¬ é•œå¤´ ${shot.shotNumber}: ${shot.timeRange}ç§’`);
    
    // æ‰“å°å¡ç‚¹ä¿¡æ¯
    const beatPointsInRange = this._getBeatPointsInRange(shot, keyframeData.storyboard?.musicAnalysis);
    if (beatPointsInRange.length > 0) {
      const relativeTimes = beatPointsInRange.map(b => `${(b - shot.startTime).toFixed(2)}s`).join(', ');
      console.log(`    ğŸµ å¡ç‚¹: ${beatPointsInRange.map(b => `${b.toFixed(2)}s`).join(', ')} (ç›¸å¯¹æ—¶é—´: ${relativeTimes})`);
    } else if (shot.beatPoint != null) {
      const relativeTime = shot.beatPoint - shot.startTime;
      console.log(`    ğŸµ å¡ç‚¹: ${shot.beatPoint.toFixed(2)}s (ç›¸å¯¹æ—¶é—´: ${relativeTime.toFixed(2)}s)`);
    }
    
    try {
      const videoPath = path.join(config.paths.temp, `shot_${shot.shotNumber}.mp4`);
      const videoPrompt = this._buildPrompt(shot, keyframeA, keyframeB, keyframeData.storyboard);
      
      // æ‰“å°å®Œæ•´æç¤ºè¯
      console.log(`\n    ğŸ“ å®Œæ•´æç¤ºè¯:`);
      console.log(`    ${videoPrompt}\n`);
      
      const referenceImages = this._prepareReferenceImages(keyframeA, keyframeB);
      
      await geminiClient.generateVideo(videoPrompt, videoPath, 'veo-3.1-generate-preview', referenceImages);
      
      material.path = videoPath;
      material.status = 'generated';
      material.prompt = videoPrompt;
      console.log(`    âœ… ç”Ÿæˆå®Œæˆ`);
    } catch (error) {
      console.error(`    âŒ ç”Ÿæˆå¤±è´¥: ${error.message}`);
      material.status = 'failed';
      material.error = error.message;
      
      // ä½¿ç”¨å…³é”®å¸§ A ä½œä¸ºåå¤‡
      if (keyframeA?.path) {
        material.path = keyframeA.path;
        material.type = 'image';
        material.status = 'keyframe_fallback';
        console.log(`    âš ï¸  ä½¿ç”¨å…³é”®å¸§ä½œä¸ºåå¤‡`);
      }
    }
  }

  // åŸºäº AB å…³é”®å¸§ç”Ÿæˆè§†é¢‘
  async generate(keyframeData) {
    console.log('ğŸ¬ Agent 5: è§†é¢‘ç”Ÿæˆå™¨ - å¼€å§‹ç”Ÿæˆ...');
    
    try {
      const keyframes = keyframeData.keyframes || [];
      console.log(`ğŸ¬ åŸºäº AB å…³é”®å¸§ç”Ÿæˆ ${keyframes.length} ä¸ªè§†é¢‘ç‰‡æ®µ...\n`);
      
      const materials = keyframes.map(kf => this._createMaterial(kf));
      
      // ä½¿ç”¨å¹¶å‘æ§åˆ¶å·¥å…·å‡½æ•°
      await batchConcurrent(materials, 
        material => this._generateVideo(material, keyframeData),
        {
          concurrency: 5,
          startIndex: 0,
          onBatchStart: (batch, batchNum, total) => {
            console.log(`\nğŸ“¦ æ‰¹æ¬¡ ${batchNum}/${total}: é•œå¤´ ${batch[0].shotNumber}-${batch[batch.length - 1].shotNumber}`);
          },
          onBatchComplete: (batch, batchNum, total) => {
            const success = batch.filter(m => m.status === 'generated').length;
            console.log(`  âœ… æ‰¹æ¬¡å®Œæˆ: ${success}/${batch.length} ä¸ªè§†é¢‘\n`);
          }
        }
      );

      const successCount = materials.filter(m => m.status === 'generated').length;
      console.log(`âœ… è§†é¢‘ç”Ÿæˆå®Œæˆ: ${successCount}/${materials.length} ä¸ªè§†é¢‘\n`);
      
      return {
        keyframeData,
        materials,
        timestamp: new Date().toISOString()
      };
    } catch (error) {
      console.error('âŒ è§†é¢‘ç”Ÿæˆå¤±è´¥:', error);
      throw error;
    }
  }
}

export default new VideoGeneratorAgent();

